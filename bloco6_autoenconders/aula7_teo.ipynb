{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AULA 7 TEORIA- Redes Auto-enconders/auto-associativas\n",
    "\n",
    "Semana 10-14/10 - Aula assíncrona\n",
    "> https://www.youtube.com/watch?v=O6C6wl357-o&ab_channel=MoacirAntonelliPonti\n",
    "\n",
    "\n",
    "Auto-encoder e tarefa de reconstrução\n",
    "Denoising Autoencoders\n",
    "Variational Autoencoders\n",
    "\n",
    "---\n",
    "\n",
    "## Regularização, Normalização e Transferência de Aprendizado\n",
    "\n",
    "- Autoenconders\n",
    "- Undercomplete\n",
    "- Overcomplete\n",
    "- Denoising Autoencoders\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders\n",
    "\n",
    "- Autoencoders são redes neurais que tem como objetivo aprender uma representação compacta dos dados de entrada.\n",
    "    - redes neurais que buscam aprender SEM RÓTULOS -> **NÃO SUPERVISIONADO**\n",
    "\n",
    "* autoenconders associam a entrada e a saída (representação compacta)\n",
    "    * x -> encoder -> x^ (representação compacta)\n",
    "    * <img src=\"encoder_decoder.jpg\" width=\"400\">\n",
    "\n",
    "- **Encoder:** aprende um *códigos* que representa a entrada (também chamado de *representação latente* ou *feature embedding*)\n",
    "    - h = s(Wx + b) = f(x)\n",
    "\n",
    "* **Decoder:** aprende a reconstruir a entrada a partir do código\n",
    "    * x^ = s(W^Th + b^T) = g(h)\n",
    "\n",
    "- exemplo: digito\n",
    "    - entrada: 28x28 = 784 pixels\n",
    "    - saída: 28x28 = 784 pixels\n",
    "    - representação latente: 10 dimensões\n",
    "    - código resistringiu em 10 o espaço de representações -> menor resolução\n",
    "    - <img src=\"digito.jpg\" width=\"300\">\n",
    "\n",
    "* saída x^ = g(f(x)), com isso minimizamos o erro na recostrução da entrada com:\n",
    "    * L(x, x^) = L(x, g(f(x)))\n",
    "    * exemplo: L(x,x^) = ||x - x^||^2 (mean squadred error)\n",
    "\n",
    "- **tipos de autoenconder:**\n",
    "    - **undercomplete:** número de neurônios na camada oculta é menor que o número de neurônios na camada de entrada\n",
    "        - a camada do código é chamada de gargalo ou \"bottleneck\" por ser restrita\n",
    "    - **overcomplete:** número de neurônios na camada oculta é maior que o número de neurônios na camada de entrada\n",
    "        - há diferentes versões desse tipo para compensar a falta de restrição do código\n",
    "\n",
    "* *por que usar um autoenconder?*\n",
    "    * a fubnção h do código ela aprende as característica de x, enquanto isso ela pode diminuir a dimensionalidade dos dados (restrição)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undercomplete\n",
    "\n",
    "- código é uma compressão cm perdas da entrada\n",
    "    - camada de código é chamada de bottheneck\n",
    "    - código produz boa representaçaõ dos dados de treinamento em particular para reconstrução\n",
    "    - exemplo:\n",
    "    - <img src=\"exemplo_undercomplete.jpg\" width=\"400\">\n",
    "    - note que ela reconstroi ruido tentando montar um padrão de digito, pq foi isso que ela aprendeu a fazer\n",
    "\n",
    "* **vantagens:**\n",
    "    * redução de dimensionalidade\n",
    "    * aprendizado de representações não supervisionado\n",
    "    * aprendizado de representações não lineares\n",
    "    * aprendizado de representações invariantes a ruído\n",
    "\n",
    "* **desvantagens:**\n",
    "    * possibilidade de decorar features muito específicas se o modelo tiver ma alta capacidade de representação\n",
    "\n",
    "- Um autoencoder denso com uma única camada encoder/decoder tem relações com o método \"Principal Component Analysis\" (PCA)\n",
    "\n",
    "* *obs:*  Os autoenconders podem ser profundos, isto é, podem adimitir camada não densas (conv, pooling, etc), porém, a camada do CÓDIGO é comumente **densa** (Fully Connected) para permitir a projeção dos dados\n",
    "\n",
    "- exemplo: UNET\n",
    "    - <img src=\"unet.jpg\" width=\"400\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overcomplete\n",
    "\n",
    "- **Overcomplete:** número de neurônios na camada oculta é maior que o número de neurônios na camada de entrada\n",
    "    - uma implementação simples permitira a cópia simples (e perfeita) dos dados de forma que x = x^\n",
    "    - <img src=\"simplees_overcomplete.jpg\" width=\"300\">\n",
    "\n",
    "* NÃO QUEREMOS CÓPIA PERFEITA, para isso, podemos utilizar uma regularização (exemplo L1)\n",
    "    * no fim, a ideia é que a funçaõ de custo tente manter um baixo numero de ativações por entrada\n",
    "    * dropout também pode ser usado, nesse caso imediatamente antes da camada do código (para impedir que ele confie na cópia como sendo perfeita)\n",
    "    * <img src=\"L1_overcompleted.jpg\" width=\"300\">\n",
    "    * note que os neuorios em preto estão zerados (ou com valores muito baixos) \n",
    "    \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising Autoencoders\n",
    "\n",
    "* regularização **adicionando ruido na entrada** -> x~ = N(x) (ruido)\n",
    "    * note que a perda é computada usando a função NÃO ruidosa x\n",
    "    * e o autoenconder tenta reconstruir x a partir de x~\n",
    "    * o encoder deve aprender a remover o ruido mantendo apenas as informações essenciais n ocódigos, permitindo que o decoder reconstrua a entrada\n",
    "    * <img src=\"denoising.jpg\" width=\"300\">\n",
    "    * obs: como há o ruido, é possivel usar overcompleted sem perigo de ele copiar a entrada\n",
    "\n",
    "- Adicioanr ruído:\n",
    "    - comulmente: Ruído Gaussiano/Normal com média 0 e variância >= 1\n",
    "    - pode ser usado outros tipos de ruído (ex: salt and pepper/impulsivo)\n",
    "        - Ruído impulsivo: atribuir zero a uma porcentagem da entrada, com probabilidade p (dropout na entrada)\n",
    "\n",
    "* técnicas de regularização também podem ser usadas em Denoising Autoencoders\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão \n",
    "\n",
    "* AEs podem ser boa escolha com dados não supervisionados para aprendizado de manifolds e agrupamento\n",
    "    * Representam uma nova tarefa: reconstrução que pode ser acoplada a outras arquiteturas\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a33b474888067a6169f866e52f630d6f3672d35114c8362b477a93e2a003ce7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
