{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AULA 10 TEORIA- Sequence-to-Sequence e Mecanismo de Atenção \n",
    "\n",
    "09/11 - Aula presencial\n",
    "\n",
    "- Métodos sequence-to-sequence\n",
    "- Atenção: cues, pooling e funções de scoring\n",
    "- Mecanismo básico de atenção\n",
    "- Multi-head attention\n",
    "- Self-attention\n",
    "\n",
    "---\n",
    "\n",
    "## Word2Vec, Sequence2Sequence e Mecanismo de Atenção\n",
    "\n",
    "- Word2Vec: representações para texto\n",
    "- Sequence to Sequence e Mecanismo de atenção\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "* Representação (embedding) para palavras\n",
    "    * função custo para aprender essa representação: p((w1, w2, ..., wn) | wt)\n",
    "    * primeiro todas as palavras do contexto de t e depois a palavra alvo\n",
    "    * otimiza em função de palavras que devem estar próximas se tiverem o mesmo **contexto**\n",
    "\n",
    "- **Skip-Grams (SG)**\n",
    "    - predição de palavras em uma certa janela de proximdiade m de uma palavra t\n",
    "    - formuçação do problema: p(wt | w1, ..., wm)\n",
    "    - softmax:\n",
    "        - <img src=\"softmax_skipgram.jpg\" width=350>\n",
    "\n",
    "* Skip gram com one-hot de uma plaavra: W * wt = vc\n",
    "    * <img src=\"onehot.jpg\" width=500>\n",
    "    * W aprende representação (nas colunas) opara cada palavra quando são centrais\n",
    "    * Uo aprende representações (nas linhas) para cada palavra quando são contexto (ou seja, quando são vizinhas)\n",
    "\n",
    "- Outros\n",
    "    - **GloVe**\n",
    "        > https://nlp.stanford.edu/projects/glove/\n",
    "    - NILC (ICMC)\n",
    "        > http://www.nilc.icmc.usp.br/embeddings\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence-to-Sequence e Mecanismo de atenção\n",
    "\n",
    "### Sequence-to-Sequence\n",
    "\n",
    "- Sequence to sequence: tradução de uma sequência para outra\n",
    "    - <img src=\"s2s_enc_dec.jpg\" width=600>\n",
    "\n",
    "\n",
    "### Mecanismo de atenção\n",
    "\n",
    "* Encontrar qual parte de uma sequência é mais importante para predizer uma certa saída\n",
    "    * Em unidades recorrentes, cada entrada perturba a memória prejudicando conhecimento de dados anteriores\n",
    "\n",
    "- Exemplos\n",
    "    - imagens\n",
    "        - <img src=\"mec_atencao.jpg\" width=300>\n",
    "        - rede faz o highlight dos mencanismo de atenção\n",
    "        - previsão tá horrível\n",
    "    - texto\n",
    "        - <img src=\"matriz_mec_atencao.jpg\" width=300>\n",
    "        - cada linha é um vetor de atenção -> matriz de atenção\n",
    "        - entende as relações entre as palavras \n",
    "\n",
    "* implementação \n",
    "    * Computar o alinhamento/similaridade entre o sumário atual do decoder, si  com sumários anteriores do encoder, hj\n",
    "    * Usa softmax para obter pesos na forma de probabilidades\n",
    "    * Atenção produz um vetor de \"contexto\" a ser usado para produzir a saída atual\n",
    "\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a33b474888067a6169f866e52f630d6f3672d35114c8362b477a93e2a003ce7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
