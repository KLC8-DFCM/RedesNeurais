{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AULA 11 TEORIA- Arquitetura Transformer e BERT\n",
    "\n",
    "16/11 - Aula assíncrona\n",
    "> https://www.youtube.com/watch?v=ksseZzTegWk&ab_channel=MoacirAntonelliPonti\n",
    "\n",
    "\n",
    "- Arquitetura Transformer\n",
    "- BERT - Bidirectional Encoder Representations from Transformers\n",
    "\n",
    "---\n",
    "\n",
    "## Transformer Networks e BERT\n",
    "\n",
    "- Arquitetura Transformer\n",
    "- BERT - Bidirectional Encoder Representations from Transformers\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Networks\n",
    "\n",
    "### RNN vs Transforme Nets\n",
    "\n",
    "*  RNNs\n",
    "    * podem não funcionar com **dependencias longas**\n",
    "    * recorrencia dificulta **computação paralela** (pontos da sequencia não podem ser porcessadas em paralelo)\n",
    "    * podem sofrer com exlosão ou desaparecimento de gradiente\n",
    "\n",
    "- Transformer Networks\n",
    "    - não tem recorrência (apenas atenção)\n",
    "    - captura dependencias longas \n",
    "    - processamento paralelo: atençaõ é invariante a permutação\n",
    "\n",
    "### Arquitetura\n",
    "\n",
    "- arquitetura\n",
    "    - <img src=\"arq_TransfNEt.jpg\" width=300>\n",
    "    - explicação na aula\n",
    "    - diferencial: **POsitional Encoding** e **Multi-Head Attention**\n",
    "\n",
    "* Multi-Head Attention (atenção)\n",
    "    - <img src=\"arq_atencao.jpg\" width=450>\n",
    "    - não só mais a combinação linear de todos elementos de entrada\n",
    "    - <img src=\"atencao.jpg\" width=450>\n",
    "    - Recuperar um valor vi para uma consulta/query q baseada numa chave/key ki\n",
    "    - A similaridade entre uma consulta e todas as chaves, ponderadas pelos valores\n",
    "    - Somar ao longo de todas as chaves/valores, produz uma distribuição de pesos relacionando consulta e todos os valores\n",
    "    - <img src=\"atencao2.jpg\" width=450>\n",
    "\n",
    "- **Transformer Huggingface**\n",
    "    - https://transformer.huggingface.co/\n",
    "    - https://huggingface.co/transformers/model_doc/transformer.html\n",
    "\n",
    "* Após transformer\n",
    "    * GPT e GPT2\n",
    "    * BERT\n",
    "    * Lambda Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT \n",
    "\n",
    "- Bidirectional Encoder Representations from Transformers\n",
    "    - https://arxiv.org/pdf/1810.04805.pdf (copilot que sugeriu essa brincadeira aqui)\n",
    "\n",
    "* Método para **pré-treinar** encoders do tipo Transformer\n",
    "    * Modelo Base:\n",
    "        * 12 camadas Transformer\n",
    "        * Embedding com 768 dimensões\n",
    "        * 110 milhões de parametros\n",
    "    * Large:\n",
    "        * 24 camadas Transformer\n",
    "        * Embedding com 1024 dimensões\n",
    "        * 340 milhões de parametros\n",
    "\n",
    "- Embeddings BERT \n",
    "    - <img src=\"embeddings_BERT.jpg\" width=500>\n",
    "    - cada pedaço da frase é separado em 3 embeddings: Token, Segment e Positional\n",
    "        - OBS: o bert não separa exatamete em palavras, ele separa os geundios (play ##ing) por exemplo\n",
    "\n",
    "* Exemplo de Embeddings: GloVe\n",
    "    * vetor fixo por palavra independente do contexto\n",
    "\n",
    "- BERT utiliza o ELMo -> olha para a setença inteira antes de atribuir o vetor\n",
    "    - usa LSTM bidirecional para criar o embedding\n",
    "    - aprende (sem labels) a predizer a prox palavra (e a anterior)\n",
    "    - BERT usa essa ideia, mas transformer\n",
    "        - LSTM bidirecional: faz uma predição do comeõ da frase pra frente e do fim da frase pra trás\n",
    "        - BERT: faz uma predição utiilizando todas as palavras da frase \n",
    "            - palavra 3: usa a palvra 1, 2, 4... N\n",
    "    - <img src=\"BERT_ElMo.jpg\" width=250>\n",
    "\n",
    "* BERT: ULM-FIT\n",
    "    * https://arxiv.org/pdf/1801.06146.pdf (copilot que sugeriu)\n",
    "    * metodos afetivos para pré trienamento para alem \n",
    "        * de words embeddings\n",
    "        * de words embeddings contextualizado\n",
    "    * modelo de linguagem + estratégia par aajustar modelo para várias tarefas\n",
    "    * descongelamento gradual: última camada até a primeira\n",
    "\n",
    "- \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a33b474888067a6169f866e52f630d6f3672d35114c8362b477a93e2a003ce7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
