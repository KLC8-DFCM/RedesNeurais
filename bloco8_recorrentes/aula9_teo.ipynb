{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AULA 9 TEORIA- Redes Recorrentes\n",
    "\n",
    "19/10 - Aula assíncrona\n",
    "> https://www.youtube.com/watch?v=OzC_vwAqRTU&ab_channel=MoacirAntonelliPonti\n",
    "\n",
    "* Redes neurais para dados sequenciais e diferentes tipos de problemas\n",
    "* Camada recorrente\n",
    "* LSTM\n",
    "* GRU\n",
    "* \"Lookback\" no treinamento de redes recorrentes\n",
    "\n",
    "---\n",
    "\n",
    "## Redes Recorrentes\n",
    "\n",
    "- Dados sequenciais (recorrência)\n",
    "- Camada recorrente básica (RNN)\n",
    "- LSTMs e GRUs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dados sequenciais (recorrência)\n",
    "\n",
    "* dados recorrentes -> depênn de um contexto anterior (ex: texto, áudio, vídeo, TEMPO...)\n",
    "\n",
    "- redes neurais convencionais não são capazes de lidar com dados sequenciais\n",
    "    - cada entrada é independente da anterior no modelo original\n",
    "    - Camadas densas e convolucionais consideram apenas o exemplo atual para computar a saída\n",
    "    - para isso -> usamos **redes recorrentes**\n",
    "\n",
    "* redes recorrentes tem \"memória\" dos dados passados -> a ordem dos elementos de entrada IMPORTA\n",
    "    * possibilidades de RNNs:\n",
    "        * 1 entrada, saida sequencial\n",
    "            * ex: imagem de entrada e saida, sequencial de palavras descevendo a imagem \"casa\", \"com\", \"sol\", \"azul\".\n",
    "        * entrada sequencial, 1 saida\n",
    "            * ex: texto com opinião e a saida é classificaçaõ de sentimento (positivo ou negativo)\n",
    "        * entrada sequencial, saida sequencial\n",
    "            * ex: tradução de texto de um idioma para outro\n",
    "\n",
    "- *obs:* em tradução de texto é interssante por um atraso, para que acumulem algumas palavras e ele não faça uma tradução literal palavra pro palavra, mas sim por um contexto local \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camada recorrente básica (RNN)\n",
    "\n",
    "* <img src=\"RNN.png\" width=\"300\">\n",
    "\n",
    "* ut = Wh * ht-1 + Wx * xt + b\n",
    "    * ht-1 = saída da camada anterior\n",
    "    * xt = entrada atual\n",
    "    * Wh = pesos da camada recorrente\n",
    "    * Wx = pesos da camada de entrada\n",
    "    * b = bias\n",
    "\n",
    "- ht = f(ut)\n",
    "    - f = função de ativação (ex: tanh, sigmoid, relu, etc)\n",
    "    - ht é chamado de variável de memória ou sumário\n",
    "\n",
    "* y = Wy * ht + by\n",
    "    * ht = saída da camada recorrente\n",
    "    * Wy = pesos da camada de saída\n",
    "    * by = bias da camada de saída\n",
    "\n",
    "- **Exemplo:** Predizer próximo caracter \n",
    "    - definimos uma codificação one-hot para os caracteres\n",
    "        - h = [1,0,0,0]\n",
    "        - e = [0,1,0,0]\n",
    "        - l = [0,0,1,0]\n",
    "        - o = [0,0,0,1]\n",
    "    - rede para predizer:\n",
    "    - <img src=\"caracter.png\" width=\"300\">\n",
    "\n",
    "### LSTM (Long Short Term Memory)\n",
    "\n",
    "> https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "- Funcionamento básico\n",
    "    - <img src=\"LSTM.png\" width=\"300\">\n",
    "    - **Cell State**: Responsável pela memória longa (contribuiçaõ ao longo da iteração anterior)\n",
    "    - **Forget State**: Decide qual elementos de C  serão esquecidos (zerados) com base no sumário anterior e a entrada atual\n",
    "        - algo entre 0 (esquecer) e 1 (manter) para cada dimensão de C\n",
    "    - **Input Gate**: Decide quais elementos de C serão atualizados (o que será adicionado)\n",
    "    - **Update State**: Atualiza o sumário com base no novo C\n",
    "    - **Output Gate**: Decide quais elementos de C (qual sumário) serão usados para gerar a saída\n",
    "        - igual uma RNN (essa parte)\n",
    "\n",
    "### GRU (Gated Recurrent Unit)\n",
    "\n",
    "* facilitar a LSTM (menos parametros e menos complexidade)\n",
    "    * proposta mais recente\n",
    "\n",
    "- Funcionamento básico\n",
    "    - <img src=\"GRU.png\" width=\"300\">\n",
    "    - não possui cell state\n",
    "    - **Reset Gate r:** filtra qual parte de ht-1 será utilizada para compor o novo sumário candidato em conjunto com xt\n",
    "    - **Update Gate z:** pondera partes do sumário anterior de forma complementar ao novo estado candidato\n",
    "    - h~t é o sumário \"candidato\"\n",
    "\n",
    "### GRU x LSTM  \n",
    "\n",
    "- não há consenso sobre qual é melhor\n",
    "    - LSTM é mais complexa e mais lenta\n",
    "    - GRU é mais simples e mais rápida\n",
    "    - GRU em muitos casos tem resultados simalar ao LSTM, só que com menos parametros\n",
    "    - ultimamente não se usa mais as RNNs clássicas\n",
    "\n",
    "* há uma versão recente, **JANET** que simplificou aidna mais o modelom removendo o \"Reset Gate\"\n",
    "    * existem também as **Temporal Convolutional Networks**, que utiliam convoliuções 1D para aprender posiconamento local de elementos sequenciais, elas também se mostram eficientes em alguns cenários\n",
    "\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a33b474888067a6169f866e52f630d6f3672d35114c8362b477a93e2a003ce7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
