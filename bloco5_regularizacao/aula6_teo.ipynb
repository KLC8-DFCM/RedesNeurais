{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AULA 6 TEORIA- egularização, Normalização e Transferência de Aprendizado\n",
    "\n",
    "05/10 - Aula presencial\n",
    "\n",
    "---\n",
    "\n",
    "## Regularização, Normalização e Transferência de Aprendizado\n",
    "\n",
    "- Generalização e complexidade de modelos\n",
    "- Overfitting/underfitting\n",
    "- Técnicas de regularização: funções de custo regularizadas, dropout\n",
    "- Early stopping\n",
    "- Data augmentation\n",
    "- Normalização por batch e camada\n",
    "- Transferência de aprendizado\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento de redes profundas em cenários reais\n",
    "\n",
    "### Suposições para convergência e aprendizado\n",
    "\n",
    "- **suposições Dados de treinamento**:\n",
    "    - Limpos\n",
    "    - Representativos e bem definidos\n",
    "    - Baixa taxa de erro de rótulo\n",
    "    - Quantidade de dados é suficiente\n",
    "\n",
    "* E se não for possível?\n",
    "    * overfitting, baixa generalização, dificuldade de treinamento\n",
    "\n",
    "- **Complexidade de modelos**:\n",
    "    - algoritmo ajusta uma f a partir de um espço de funções F\n",
    "        - *\"muitas\" funções:* mais graus de liberdade, menor garantia de convergência, possível overfitting\n",
    "        - *\"poucas\" funções:* menos graus de liberdade, maior garantia de convergência, possível underfitting\n",
    "        - <img src=\"espaco_funcoes.png\" width=\"300\"> \n",
    "\n",
    "* ou seja:\n",
    "    * mais funções -> mais opções para conter a função ótima do problema, porém, mais dificil será achar a melhor função do conjunto escolhido.\n",
    "    * menos funções -> mais fácil achar a melhor função do conjunto, porém, talvez ela não seja ótima o suficiente\n",
    "\n",
    "- obs: complexidade de modelos = \"viés\" segundo a Teoria do Aprendizado Estatístico\n",
    "\n",
    "* exemplo com KNN (K nearst neighboughrsj):\n",
    "    * ver slide (ta sem explicacao mas era sobre overfitting e underfitting)\n",
    "\n",
    "- **viés x variância**:\n",
    "    - complexidade baixa: pouca variancia e muito viés\n",
    "        - underfitting\n",
    "    - complexidade alta: muita variancia e pouco viés\n",
    "        - overfitting\n",
    "    - complexidade ótima: erro de generalizaçaõ minimo\n",
    "    - <img src=\"vies_variancia.png\" width=\"400\"> \n",
    "\n",
    "* **overparametrization**:\n",
    "    * expansão do conceito anterior -> dependencia dos parametros\n",
    "        * proposta inteira EMPÍRICA (sem comprovaçaõ matemática ainda)\n",
    "    * a rede enquanto \"underparametrized\", quanto mais complexidade de funções, mais ela DECORA o dataset, até que chegamos no \"interpolation threshhold\", em que a rede decora tudo que há de dataset e a partir daí aprende a interpolar as respostas com o dataset decorado. \n",
    "        * a rede volta aprender e reduzir seu regime, voltando a ter uma generalização aceitável \n",
    "    * <img src=\"overparametrization.png\" width=\"400\"> \n",
    "\n",
    "- **Hipótese do bilhete de loteria**:\n",
    "    - inicializar aleatoriamente uma rede densa com Θo\n",
    "    - treinar a rede até atingir convergência com parâmetros Θ\n",
    "    - Podar Θ e criar uma máscara m\n",
    "    - A configuração de inicialização “winning ticket” é Θ ⊗ m (produto interno)\n",
    "\n",
    "* **Ataques adversariais**:\n",
    "    * enganar a rede neural com alguns pixels e coódigos de pixels que traduzem features aprendidas de forma \"artifical\", podendo confundir a rede \n",
    "    * <img src=\"NN_fooled.png\" width=\"400\">\n",
    "    * <img src=\"pixel_attack2.png\" width=\"400\">\n",
    "    * note que adicionar um ponto em cada imagem era muito influente na rede, de tal forma que o ponto branco no centro deu para a rede a certeza falsa de que era um avião\n",
    "\n",
    "- Zhang et al (2017): \"... nossos experimentos estabeleceram que redes convolucionais profundas do estado da arte (...) facilmente ajustam rótulos aleatórios nos dados de treinamento.\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estratégias para melhorar a generalização\n",
    "\n",
    "- Regularização\n",
    "    - Regularização L2\n",
    "    - Regularização L1\n",
    "    - Dropout\n",
    "    - Early stopping\n",
    "    - Data augmentation\n",
    "    - Normalização por batch e camada\n",
    "    - Transferência de aprendizado\n",
    "\n",
    "* **[I]: Relgularização L2 (Tikonox)**:\n",
    "    * <img src=\"regularizacaoL2.png\" width=\"250\"> \n",
    "    * Objetivo: limitar a capacidade do modelo de se especializar demais nos dados (decorar o dataset)\n",
    "    * Regularização: \n",
    "        * Global -> na função de perda ponderada por um hiperparâmetro λ\n",
    "        * Definido -> na função de perda de cada camada\n",
    "\n",
    "- **[II]: Dropout**:\n",
    "    - <img src=\"dropout.png\" width=\"300\"> \n",
    "    - Objetivo: limitar a capacidade de certos parâmetros do modelo a memorizarem os dados\n",
    "    - implementado na forma de \"camada\"\n",
    "        - em cada iteração desliga aleatoriamente uma porção dos neurônios da camada\n",
    "\n",
    "* **[III]: Early Stopping**:\n",
    "    * Objetivo: evitar que o modelo se especialize demais nos dados de treinamento \n",
    "    * Parar o treinamento quando a função de custo de validação não melhora mais\n",
    "\n",
    "- **[IV]: Coletar mais dados**:\n",
    "    - Objetivo: aumentar a quantidade de dados de treinamento e impedir que o treinamento considere apenas um conjunto limitado de exemplos\n",
    "    - Baseado na lei dos grandes números, quanto maior a amostra,teremos um melhor estimador\n",
    "\n",
    "* **[V]: Data Augmentation**:\n",
    "    * Objetivo: aumentar a quantidade de dados de treinamento *> gerar exemplos artificiais na *esperança* de que melhore as propriedades de convergência\n",
    "    * Gera novos dados a partir de transformações aleatórias nos dados de treinamento\n",
    "    * Implementado por meio da manipulação de exemplos existentes, ou sua combinação\n",
    "    * Exemplos:\n",
    "        * rotação\n",
    "        * zoom\n",
    "        * translação\n",
    "        * espelhamento\n",
    "        * mudança de brilho\n",
    "        * mudança de contraste\n",
    "        * Dropout na camada de entrada: eliminando features caleatoriamente a cada iteração\n",
    "        * etc\n",
    "\n",
    "- *obs: Dica para melhoria de performance final*\n",
    "    - para cada exemplo de teste:\n",
    "        - 1) Gerar m exemplos com data augmentation\n",
    "        - 2) Predizer o resultado dos m exemplos\n",
    "        - 3) Combinar as predições (média, maioria etc)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalização de dados\n",
    "\n",
    "* exemplos de normalização:\n",
    "    * **Normalizaçaõ z-score:** valores com média 0 e desvio padrão 1\n",
    "    * **Normalizaçaõ min-max:** valores entre 0 e 1\n",
    "    * **Normalizaçaõ por batch:** normaliza os dados de cada batch de forma independente\n",
    "\n",
    "- Objetivo: evitar que os dados de entrada sejam muito grandes ou muito pequenos -> evitar que o gradiente exploda ou desapareça -> otimização \n",
    "    - suaviza as ativações dos neurônios, reduzindo a varianca do gradiente\n",
    "    - ataca o problema de \"vanishing gradient\"\n",
    "\n",
    "* tipos de normalização baseada em camadas!\n",
    "    * batch \n",
    "    * camada \n",
    "    * instância\n",
    "\n",
    "- Normalização de Dados\n",
    "    - Normalização por batch\n",
    "    - Normalização por camada\n",
    "    - Normalização por instância\n",
    "    - <img src=\"normalizacao.png\" width=\"400\">\n",
    "\n",
    "* **Batch normalization (BN):** para cada batch\n",
    "    * média e desvio calculados por canal (total C)\n",
    "    * normalização por canal (ao longo de N instancias do batch)\n",
    "    * funciona melhor com batchsize > 32\n",
    "\n",
    "- **Layer normalization (LN):** para cada camada\n",
    "    * média e desvio calculados por instancia (total N)\n",
    "    * normalização por instancia (ao longo de todas as ativações do batch)\n",
    "    * Independe do tamanho do batch, mais comum em redes recorrentes e adversariais\n",
    "\n",
    "- **Instance normalization (IN):** para cada instância\n",
    "    * média e desvio calculados por instancia e canal (total N*C)\n",
    "    * normalização por instancia (ao longo de cada canal)\n",
    "    * Independe do tamanho do batch, mais comum em redes recorrentes e adversariais\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transferência de aprendizado\n",
    "\n",
    "- **Transferência de aprendizado:** Utilizar modelo treinado em uma determinada tarefa ou domínio, aproveitando o aprendizado para uma outra tarefa ou domínio alvo\n",
    "    - **Transferência de aprendizado entre tarefas:** transferir o conhecimento de uma tarefa para outra\n",
    "    - **Transferência de aprendizado entre domínios:** transferir o conhecimento de um domínio para outro\n",
    "\n",
    "* Modos mais comuns:\n",
    "    * ajuste-fino/ adaptação de parâmetros\n",
    "    * transferência de aprendizado por *feature extraction*\n",
    "\n",
    "- **Ajuste-fino/ adaptação de parâmetros:**\n",
    "    - Transferencia de aprendizado\n",
    "        - Inicializar o modelo com os pesos pré-treinados\n",
    "        - treinamento a partir dos pesos pré-treinados\n",
    "        - Permitir adaptação apenas da últimas camadas, congelando as demais\n",
    "    - Ajuste fino\n",
    "        - Inicializar o modelo com os pesos pré-treinados (feito após o anterior)\n",
    "        - treinamento a partir dos pesos pré-treinados\n",
    "        - Permitir adaptação de todos os pesos (principalmente das ultimas camadas até o meio, do começo depende da disponibilidade de dados)\n",
    "    - <img src=\"ajuste_fino.png\" width=\"300\">\n",
    "\n",
    "* *Dicas*: \n",
    "    * CNNs com menos parâmetros costumam generalizar melhor para dados muito diferentes do treinamento\n",
    "    * Exemplos: MobileNet, SqueezeNet, etc. funcionam melhor em imagens médicas do que ResNet e Inception.\n",
    "    * Ajuste-fino pode não convergir se tivermos poucos dados, ex.menos de 100 instâncias por classe.\n",
    "\n",
    "- **Transferência de aprendizado por *feature extraction*:**\n",
    "    - Características para dados não estruturados\n",
    "    - Carregar rede neural treinada em grande base de dados\n",
    "    - Passar exemplos de sua base de dados pela rede para predição (não treinamento!)\n",
    "    - Obter os mapas de ativação de alguma camada\n",
    "    - <img src=\"feature_extraction.png\" width=\"300\">\n",
    "\n",
    "* *Dicas*:\n",
    "    * Aplicar redução de dimensionalidade baseada em PCA,Product Quantization ou outra\n",
    "    * Treinar modelo de aprendizado raso com maiores garantias de aprendizado com poucos dados: SVM, árvore de decisão, etc\n",
    "    * Essas características também são efetivas para recuperação baseada em conteúdo\n",
    "    * Podem ser usados métodos de projeção para as características aprendidas: tSNE, UMAP, PCA\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n",
    "- Deep Learning não pode ser tratado como panacéia\n",
    "- Há ainda preocupações sobre sua capacidade de generalização\n",
    "- Grande utilidade está no aprendizado de representações em particular para dados não estruturados\n",
    "    - representações que parecem ter excelente cpaacidade de transferẽncia de aprendizado\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a33b474888067a6169f866e52f630d6f3672d35114c8362b477a93e2a003ce7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
