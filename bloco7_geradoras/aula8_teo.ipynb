{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AULA 8 TEORIA- Modelos geradores\n",
    "\n",
    "19/10 - Aula assíncrona\n",
    "> https://www.youtube.com/watch?v=1LLDe8VheUs&ab_channel=MoacirAntonelliPonti\n",
    "> https://www.youtube.com/watch?v=1jjHO5_8lQk&ab_channel=MoacirAntonelliPonti\n",
    "\n",
    "\n",
    "* Redes Geradoras\n",
    "* Autoencoders Variacionais\n",
    "* Redes adversariais: Discriminador e Gerador\n",
    "* Redes baseadas em Difusão\n",
    "\n",
    "---\n",
    "\n",
    "## Redes Geradoras\n",
    "\n",
    "- Modelos geradores\n",
    "- Autoencoders variacionais (VAEs)\n",
    "- Redes adversárias geradoras (GANs)\n",
    "- Modelos baseados em difusão\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos geradores\n",
    "\n",
    "* Redes geradoras são modelos que aprendem a gerar novos dados\n",
    "    * tipo aquelas que geram faces de pessoas a partir de fotos reais\n",
    "\n",
    "- rede tenta aprender a distribuição que \"gera\" os dados para amostrar novos dados a partir dela\n",
    "    - aprender como é a distribuição dos dados, no exemplo é tipo uma gaussiana, logo a maior densidade de dados é no pico dela, e a menor nas extremidades\n",
    "    - <img src=\"distribuicao_dados.png\" width=\"400\">\n",
    "\n",
    "* **tipos de métodos:**\n",
    "    * **Função de densidade explítica:** buscar a função densidade igual no exemplo acima\n",
    "        * Fully Visible Belief Networks (FVBNs)\n",
    "        * Boltzmann Machines (BM)\n",
    "        * Variational Autoencoders (VAEs) - *mais comum*\n",
    "    * **Função de densidade implícita:** aprender a gerar dados sem saber a distribuição (intui a forma que os dados são gerados)\n",
    "        * Monte Carlo\n",
    "        * Likehood-free inference via classification\n",
    "        * Generative Adversarial Networks (GANs) - *mais comum*\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders variacionais (VAEs)\n",
    "\n",
    "* relembrando: autoenconders tradicionais tentam codificar atributos de forma discreta, e depois decodificar para reconstruir uma estimativa do dado original\n",
    "    * obs: eles aprendem os atributos da imgem principal, e não a imagem em si (exemplo sorriso, cor da pele...) \n",
    "    * <img src=\"autoencoder2.png\" width=\"500\">\n",
    "    * *disentaglement: separar os atributos da imagem principal* \n",
    "\n",
    "- VAEs são autoencoders que aprendem a distribuição dos dados (mesma ideia acima)\n",
    "    - <img src=\"vae.png\" width=\"500\">\n",
    "    - para cada atributo, dado pelo encoder a rede vai tentar aprender a média e desvio padrão da distribuição\n",
    "    - vai ser amostrado um dado dessa distribuição \n",
    "    - essa amostra será decodificada pelo decoder\n",
    "\n",
    "* diferenças: o autoencoder codifica a imagem e tenta reconstruir ela a partir dos proprios dados, decodificando esses dados\n",
    "    * o VAE codifica a imagem e tenta reconstruir ela a partir de uma amostra da distribuição dos dados que a rede está tentando aprender\n",
    "    * <img src=\"VAE_matematicamente.png\" width=\"300\">\n",
    "\n",
    "- depois que a rede já estiver boa, é possível assumir as distribuições aprendidas são boas e podemos jogar a parte do codificador fora\n",
    "    - assim, podemos só a partir de amostrar da distribuição, gerar novos dados\n",
    "\n",
    "* Função custo ELBO (Evidence Lower Bound)\n",
    "    * L = reconstrução + divergência Kullback-Leibler (KL)\n",
    "    * L = MSE/Binary_Cross_Entropy + KL\n",
    "\n",
    "- a divergença KL é uma medida de similaridade entre duas distribuições\n",
    "    - enquanto a reconstrução tenta olhar pros valores específicos de entrada e saída, a divergencia KL olha pras distribuições inteiras\n",
    "    - <img src=\"ELBO.png\" width=\"300\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes adversárias geradoras (GANs)\n",
    "\n",
    "- adversárias: componentes que disputam entre si\n",
    "\n",
    "* Gerador G: redecebe um exemplo z' obtido da distribuição e gera x por meio de uma função:\n",
    "    * x = G(z')\n",
    "    * G projeta no espaço x um ponto específico de z (z -G-> x)\n",
    "\n",
    "- Discriminador D\n",
    "    - recebe um exemplo x e tenta classificar se ele é da distribuição original ou pela aproximaão do gerador (real ou artificial)\n",
    "\n",
    "* Paralelo: Casa da moeda e o ladrão\n",
    "    * casa da moeda e ladrão geram células, cabe a polícia descobrir qual é real e qual é falsa -> punição ou melhoria...\n",
    "    * <img src=\"moeda.png\" width=\"400\">\n",
    "    * <img src=\"GAN.png\" width=\"400\">\n",
    "    * duas fontes: gerador (artificial) e dados de treinamento (real) -> discriminador tenta definir qual é qual\n",
    "\n",
    "- formulação clássica:\n",
    "    - <img src=\"formula.png\" width=\"300\">\n",
    "    - maximizar D e minimar G (minimax)\n",
    "    - primeiro termo WEx[ log(D(x))]: \n",
    "        - referente aos dados reais (x)\n",
    "        - entropia cruzada: 1 * log(D(X)), onde 1 é o valor esperado de D(x) (se for real)\n",
    "    - segundo termo: Ez[ log(1 - D(G(z)))]:\n",
    "    \n",
    "        - referente aos dados gerados (G(z))\n",
    "        - entropia cruzada: 1 * log(1 - D(G(z))\n",
    "        - x^ = G(z) é o dado gerado\n",
    "\n",
    "* a perda clássica acima causa um comportamento muito ocilatório na função de perda, fazendo com que **seja dificl otimizar GANs**\n",
    "\n",
    "- aritimetica de esppaço latente\n",
    "    - <img src=\"aritmetica.png\" width=\"400\">\n",
    "    - sem mulheres com oculos no espaço, é possível gerar esse tipo de imagem -> pegar as features HOMEM e OCULOS, subtrair HOMEM SEM OCULOS e adcionar MULHER SEM OCULOS\n",
    "        - aritimetica: HOMEM - HOMEM cancela, SEM OCULOS - SEM OCULOS cancela, sobra MULHER COM OCULOS\n",
    "        - isso tuod com as features extraidas que se comportam como esses atributos\n",
    "\n",
    "* obs: as imagens \"as vezes\" são realistas, elas chegam a gerar boas imagens, mas também algumas muito ruins\n",
    "    * as GANs não são boas em entender varios padrões/classes diferentes em uma mesma imagem\n",
    "\n",
    "- modelos relevantes: SAGAN (2019), StyleGAN (2021), Diffusion-based Model (2021)...\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos baseados em difusão\n",
    "\n",
    "* adicionar ruido em um dado até ele se perder por completo\n",
    "    * difusão dos features de entrada até virar puro ruido\n",
    "    * x0 (imagem rea) -> xT (puro ruido)\n",
    "\n",
    "\n",
    "- ideia similar do VAE: codificar e decodificar\n",
    "    - no caso é a difusão foward e backward (codificar e decodificar)\n",
    "    - <img src=\"forward_difusao.png\" width=\"250\">\n",
    "    -  <img src=\"backward_difusao.png\" width=\"250\">\n",
    "\n",
    "* diferença da difusão e VAE: \n",
    "    * VAE: codifica e decodifica a partir de uma amostra da distribuição\n",
    "    * difusão: codifica e decodifica a partir de um dado específico\n",
    "        * difusão não precisa aprender a CODIFICAR, só a decodificar, enquanto VAEs precisam aprender os dois\n",
    "        * <img src=\"VAExdifusao.png\" width=\"400\">\n",
    "\n",
    "- perda nos modelos de difusão (não tem mais reconstrução e KL):\n",
    "    - olha basicamente pro processo de remoção de ruido da imagem\n",
    "    - <img src=\"loss_difusao.png\" width=\"250\">\n",
    "\n",
    "* modelo mais importante atualmente: BigGAN\n",
    "    * <img src=\"bigGAN.png\" width=\"400\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTMs e GRUs\n",
    "\n",
    "* pra um tempo curto o sumário consegue segurar bem a memória (curto prazo), pra longo prazo é necessário usar uma LSTM\n",
    "    * Long Short Termo Unit (LSTM)\n",
    "    * <img src=\"LSTM.png\" >\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a33b474888067a6169f866e52f630d6f3672d35114c8362b477a93e2a003ce7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
